{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86b796f-91e8-4bef-8ae5-0707ebd0ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. Extract Sample document and apply following document preprocessing methods:\n",
    " #   Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    " # 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc49dc81-2968-4c37-bcf8-749b5c1dbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028ef306-8f9b-420e-b3d4-4d1a016a4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "text ='Real madrid is set to win the UCL for the season. Benzema might win Balon dor. Salah might be the runner up. playing. original. good. better. caring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d70b7c9-9101-4471-859b-c604cf704074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real madrid is set to win the UCL for the season.', 'Benzema might win Balon dor.', 'Salah might be the runner up.', 'playing.', 'original.', 'good.', 'better.', 'caring']\n",
      "Number of sentence: 8\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "#1. sentense tokenization\n",
    "tokens_sents = nltk.sent_tokenize(text)\n",
    "print(tokens_sents)\n",
    "print(\"Number of sentence:\", len(tokens_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64aa0cc6-3158-4ea6-9403-1169e8b0d3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real', 'madrid', 'is', 'set', 'to', 'win', 'the', 'UCL', 'for', 'the', 'season', '.', 'Benzema', 'might', 'win', 'Balon', 'dor', '.', 'Salah', 'might', 'be', 'the', 'runner', 'up', '.', 'playing', '.', 'original', '.', 'good', '.', 'better', '.', 'caring']\n",
      "Number of words: 34\n"
     ]
    }
   ],
   "source": [
    "# 2.word tokenization\n",
    "tokens_words = nltk.word_tokenize(text)\n",
    "print(tokens_words)\n",
    "print(\"Number of words:\", len(tokens_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68ffb44-2cf6-4442-94d1-d3d5cf6f9fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#stop word removal\n",
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7580b328-5f3e-439e-ad4f-2123ee94684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real madrid set win UCL season. Benzema might win Balon dor. Salah might runner up. playing. original. good. better. caring\n"
     ]
    }
   ],
   "source": [
    "#removing stop words from text\n",
    "words = []\n",
    "for word in text.split():\n",
    "    if word.lower() not in sw_nltk:\n",
    "        words.append(word)\n",
    "        \n",
    "new_text = \" \".join(words)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665efcfe-a25a-437a-89e9-013bd249e14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Real madrid is set to win the UCL for the season . Benzema might win Balon dor . Salah might be the runner up. playing. original. good. better. caring\n",
      "\n",
      "Stemming (Porter): ['real', 'madrid', 'is', 'set', 'to', 'win', 'the', 'ucl', 'for', 'the', 'season', '.', 'benzema', 'might', 'win', 'balon', 'dor', '.', 'salah', 'might', 'be', 'the', 'runner', 'up', '.', 'play', '.', 'origin', '.', 'good', '.', 'better', '.', 'care']\n",
      "\n",
      "Stemming (Lancaster): ['real', 'madrid', 'is', 'set', 'to', 'win', 'the', 'uc', 'for', 'the', 'season', '.', 'benzem', 'might', 'win', 'balon', 'dor', '.', 'salah', 'might', 'be', 'the', 'run', 'up', '.', 'play', '.', 'origin', '.', 'good', '.', 'bet', '.', 'car']\n",
      "\n",
      "Stemming (Snowball): ['real', 'madrid', 'is', 'set', 'to', 'win', 'the', 'ucl', 'for', 'the', 'season', '.', 'benzema', 'might', 'win', 'balon', 'dor', '.', 'salah', 'might', 'be', 'the', 'runner', 'up', '.', 'play', '.', 'origin', '.', 'good', '.', 'better', '.', 'care']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text\n",
    "text = 'Real madrid is set to win the UCL for the season . Benzema might win Balon dor . Salah might be the runner up. playing. original. good. better. caring'\n",
    "tokens_words = word_tokenize(text)\n",
    "\n",
    "# Stemming using Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "stem_words_porter = []\n",
    "\n",
    "for word in tokens_words:\n",
    "    stem_word = ps.stem(word)\n",
    "    stem_words_porter.append(stem_word)\n",
    "\n",
    "# Stemming using Lancaster Stemmer\n",
    "ls = LancasterStemmer()\n",
    "stem_words_lancaster = []\n",
    "\n",
    "for word in tokens_words:\n",
    "    stem_word = ls.stem(word)\n",
    "    stem_words_lancaster.append(stem_word)\n",
    "\n",
    "# Stemming using Snowball Stemmer\n",
    "ss = SnowballStemmer(\"english\")\n",
    "stem_words_snowball = []\n",
    "\n",
    "for word in tokens_words:\n",
    "    stem_word = ss.stem(word)\n",
    "    stem_words_snowball.append(stem_word)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"\\nStemming (Porter):\", stem_words_porter)\n",
    "print(\"\\nStemming (Lancaster):\", stem_words_lancaster)\n",
    "print(\"\\nStemming (Snowball):\", stem_words_snowball)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c05ef6-0f98-487a-8df2-94c23e55463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real madrid is set to win the UCL for the season . Benzema might win Balon dor . Salah might be the runner up . playing . original . good . better . caring \n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = ''\n",
    "\n",
    "for word in tokens_words:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    lemmatized_output += lemma + ' '\n",
    "\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b7df27a-24f1-4a82-8531-7514df01a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real', 'madrid', 'is', 'set', 'to', 'win', 'the', 'UCL', 'for', 'the', 'season', '.', 'Benzema', 'might', 'win', 'Balon', 'dor', '.', 'Salah', 'might', 'be', 'the', 'runner', 'up', '.', 'playing', '.', 'original', '.', 'good', '.', 'better', '.', 'caring']\n"
     ]
    }
   ],
   "source": [
    "leme=[]\n",
    "for i in tokens_words:\n",
    "  lemetized_word=lemmatizer.lemmatize(i)\n",
    "  leme.append(lemetized_word)\n",
    "print(leme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dcf09e3-c9d3-43a4-880b-a66f600862dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech:  [('real', 'JJ'), ('madrid', 'NN'), ('is', 'VBZ'), ('set', 'VBN'), ('to', 'TO'), ('win', 'VB'), ('the', 'DT'), ('ucl', 'NN'), ('for', 'IN'), ('the', 'DT'), ('season', 'NN'), ('.', '.'), ('benzema', 'NN'), ('might', 'MD'), ('win', 'VB'), ('balon', 'NN'), ('dor', 'NN'), ('.', '.'), ('salah', 'NN'), ('might', 'MD'), ('be', 'VB'), ('the', 'DT'), ('runner', 'NN'), ('up', 'RB'), ('.', '.'), ('play', 'NN'), ('.', '.'), ('origin', 'NN'), ('.', '.'), ('good', 'JJ'), ('.', '.'), ('better', 'RBR'), ('.', '.'), ('care', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Part of Speech Tagging\n",
    "print(\"Parts of Speech: \",nltk.pos_tag(leme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de5abb9-6a27-4bf3-9345-12e1dbe189a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Spacy- doing all at ones\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#Spacy- doing all at ones\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = 'Google is looking at buying U.K. startup for $1 billion'\n",
    "doc = nlp(text)\n",
    "pd.DataFrame([[t.text, t.is_stop, t.lemma_, t.pos_]\n",
    "              for t in doc],\n",
    "             columns=['Token', 'is_stop_word','lemma', 'POS'])\n",
    "\n",
    "\n",
    "# Name Entity Recognition\n",
    "text = 'Google is looking at buying U.K. startup for $1 billion'\n",
    "for entity in nlp(text).ents:\n",
    "    print(\"Entity: \", entity.text)\n",
    "    print(\"Entity Type: %s | %s\" % (entity.label_, spacy.explain(entity.label_)))\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5604e7ad-d320-456c-aadb-83d22797e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names: ['china' 'data' 'earning' 'google' 'jumps' 'plunge' 'price' 'stock'\n",
      " 'today']\n",
      "Shape of TF-IDF matrix: (2, 9)\n",
      "TF-IDF matrix (dense representation):\n",
      " [[0.         0.29017021 0.4078241  0.29017021 0.4078241  0.\n",
      "  0.4078241  0.4078241  0.4078241 ]\n",
      " [0.57615236 0.40993715 0.         0.40993715 0.         0.57615236\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "#TfidfVectorizer is a class from the scikit-learn library (sklearn) \n",
    "# used for converting a collection of raw documents into a matrix of TF-IDF\n",
    "# (Term Frequency-Inverse Document Frequency) features. \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sentences = [\n",
    "'The stock price of google jumps on the earning data today',\n",
    "'Google plunge on China Data!'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "TFIDF = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Display feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"feature names:\", feature_names)\n",
    "\n",
    "# Display TF-IDF matrix information\n",
    "print(\"Shape of TF-IDF matrix:\", TFIDF.shape)\n",
    "print(\"TF-IDF matrix (dense representation):\\n\", TFIDF.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e087ef-e621-4e54-8942-6bcaea5e7450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['china' 'data' 'earning' 'google' 'jumps' 'plunge' 'price' 'stock'\n",
      " 'today']\n",
      "[[0 1 1 1 1 0 1 1 1]\n",
      " [1 1 0 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# It counts the occurrences of each word (token) in each document \n",
    "# and represents the data in a matrix format.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sentences = [\n",
    "'The stock price of google jumps on the earning data today',\n",
    "'Google plunge on China Data!'\n",
    "]\n",
    "# CountVectorizer for TF\n",
    "tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_matrix = tf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Display feature names and TF matrix\n",
    "print(tf_vectorizer.get_feature_names_out())\n",
    "print(tf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2856d72-1464-4051-80e6-d82717834ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.29017021 0.4078241  0.29017021 0.4078241  0.\n",
      "  0.4078241  0.4078241  0.4078241 ]\n",
      " [0.57615236 0.40993715 0.         0.40993715 0.         0.57615236\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# TfidfTransformer for IDF\n",
    "idf_transformer = TfidfTransformer()\n",
    "idf_matrix = idf_transformer.fit_transform(tf_matrix)\n",
    "\n",
    "# Display IDF matrix\n",
    "print(idf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "958edbc0-eb57-4bbd-8abc-1b33c9f178d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['china' 'data' 'earning' 'google' 'jumps' 'plunge' 'price' 'stock'\n",
      " 'today']\n",
      "[[0.         0.29017021 0.4078241  0.29017021 0.4078241  0.\n",
      "  0.4078241  0.4078241  0.4078241 ]\n",
      " [0.57615236 0.40993715 0.         0.40993715 0.         0.57615236\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# First, compute the IDF matrix\n",
    "idf_transformer = TfidfTransformer()\n",
    "idf_matrix = idf_transformer.fit_transform(tf_matrix)\n",
    "\n",
    "# Now, combine TF and IDF for TF-IDF matrix\n",
    "tfidf_matrix = tf_matrix.multiply(idf_matrix)\n",
    "\n",
    "# Display feature names and TF-IDF matrix\n",
    "print(tf_vectorizer.get_feature_names_out())\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a861db5-631f-4231-b992-d644b8eb588c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
